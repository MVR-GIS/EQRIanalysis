# Copilot Chat Conversation Export: Factor analysis workplan for EQRI project

**User:** @mpdougherty
**Thread URL:** https://github.com/copilot/c/d0b75410-5388-4884-bf52-b97ca02e4e76

## @mpdougherty

This Engineering Quality Risk Indicators project asks engineers to respond to questions believed by experts to be drivers of enginnering quality. I need to perform response analysis of the first year's data to evaluate the following: Are we asking the right set of questions (too many, too few, gaps, too much detail in one area)? How senstitve are indicators to the questions included in that metric (question influence, do we need to incorporate weightings)? What has been accomplished so far: 1. Constructed a Quarto website to visualize results for consumption by analysis team and reviewers. An embedded R package of functions to perform the analysis following reproducible research best practice. Exploratory Data Analysis (EDA)-style descriptive statistics. Question reliability analysis. Considering the questions raised above and the work already completed, create a proposed workplan analysis to accomplish the following remaining known todo items: factor analysis. Follow industry best practice to suggest things I am missing. 

## Copilot

I'll search the repository to understand the current state of your EQRI analysis project and then provide a comprehensive workplan.Let me gather more specific information about the project structure and existing analyses.Based on my analysis of your EQRI project repository, I'll create a comprehensive workplan for your remaining factor analysis work and additional analysis considerations. Let me first surface the most relevant documentation for the methods you'll need.

## **Recommended Documentation References**

Before proceeding with the workplan, here are the essential resources aligned with best practice:

### **Factor Analysis Resources**
1. **`psych` package** (already in your DESCRIPTION): 
   - [CRAN psych documentation](https://cran.r-project.org/web/packages/psych/index.html)
   - [Vignette: An overview of psych procedures](https://cran.r-project.org/web/packages/psych/vignettes/overview.pdf)
   - Your `calculate_omega()` function already uses `psych::omega()` which performs hierarchical factor analysis

2. **Exploratory Factor Analysis (EFA)**:
   - Revelle, W. (2024). *psych: Procedures for Psychological, Psychometric, and Personality Research*
   - [Factor Analysis Tutorial](https://personality-project.org/r/psych/HowTo/factor.pdf)

3. **Confirmatory Factor Analysis (CFA)** - if needed:
   - `lavaan` package: https://lavaan.ugent.be/
   - Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. *Journal of Statistical Software*, 48(2), 1-36.

### **Survey/Questionnaire Validation Best Practices**
4. **DeVellis, R. F. (2016)**. *Scale Development: Theory and Applications* (4th ed.). SAGE Publications.
   - Chapter 7: Guidelines for scale development and validation

5. **American Educational Research Association, American Psychological Association, & National Council on Measurement in Education (2014)**. *Standards for Educational and Psychological Testing*.
   - Standards for validity evidence based on internal structure

---

## **Proposed Analysis Workplan**

### **Phase 1: Dimensionality Assessment (Factor Analysis Foundation)**

**Objective**: Determine the optimal factor structure underlying your EQRI questionnaire

#### **1.1 Assess Factorability**
- [ ] **Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy**
  - Function: `psych::KMO()`
  - Minimum acceptable: 0.60; Good: > 0.80
  - Calculate for overall questionnaire and by context (program × milestone)

- [ ] **Bartlett's Test of Sphericity**
  - Function: `psych::cortest.bartlett()`
  - Should be significant (p < .05) to proceed with factor analysis

#### **1.2 Determine Number of Factors**
- [ ] **Parallel Analysis** (Gold standard)
  - Function: `psych::fa.parallel()`
  - Compares eigenvalues from actual data to random data
  
- [ ] **Scree Plot Examination**
  - Visual inspection for "elbow"
  - `psych::fa.parallel()` produces this automatically

- [ ] **Theoretical Considerations**
  - How many indicators were you designed to measure?
  - Review the conceptual groupings from your expert panel

#### **1.3 Exploratory Factor Analysis (EFA)**
- [ ] **Run EFA with different extraction methods**
  - Minimum residual (`fm = "minres"`) - your current default in `calculate_omega()`
  - Principal axis (`fm = "pa"`)
  - Maximum likelihood (`fm = "ml"`)

- [ ] **Try different rotation methods**
  - Oblique rotation (assumes factors correlate): `"oblimin"` or `"promax"` (recommended for most questionnaires)
  - Orthogonal rotation (assumes uncorrelated factors): `"varimax"`

- [ ] **Evaluate model fit**
  - Tucker-Lewis Index (TLI) > 0.90
  - Root Mean Square Error of Approximation (RMSEA) < 0.08
  - Root Mean Square of Residuals (RMSR) < 0.05

**Implementation Suggestion**:
```r
# Create function similar to your existing structure
run_factor_analysis <- function(responses_df, 
                               program_name, 
                               milestone_name,
                               nfactors = NULL,
                               fm = "minres",
                               rotate = "oblimin") {
  # Use your existing get_wide_responses() infrastructure
  # Add parallel analysis if nfactors is NULL
  # Return structured results list
}
```

---

### **Phase 2: Question Sensitivity & Influence Analysis**

**Objective**: Understand which questions drive indicator scores and whether weighting is needed

#### **2.1 Item-Total Correlations**
- [ ] **Corrected item-total correlations**
  - You already capture this in `calculate_cronbach_alpha()` via `alpha_result$item.stats`
  - Minimum acceptable: r > 0.30
  - Flag items < 0.30 as weak contributors

#### **2.2 Factor Loadings Analysis**
- [ ] **Examine factor loadings from EFA**
  - Loadings > 0.40 generally considered meaningful
  - Items with cross-loadings (>0.32 on multiple factors) may be problematic
  - Pattern of loadings indicates which questions define each indicator

- [ ] **Create visualization**
  - Heat map of factor loadings by question
  - Similar style to your existing `plot_reliability_comparison()`

#### **2.3 Indicator Sensitivity Analysis**
- [ ] **Calculate indicator scores with different weighting schemes**
  - **Unweighted** (current approach): Simple mean of item responses
  - **Factor-score weighted**: Use regression scores from factor analysis
  - **Reliability-weighted**: Weight by item-total correlation
  
- [ ] **Compare indicator scores across weighting methods**
  - Correlation between methods
  - Impact on ranking/classification of projects
  - If correlations > 0.90, weighting may not be necessary

**Implementation Suggestion**:
```r
compare_weighting_methods <- function(responses_df, 
                                     indicator_name,
                                     factor_loadings) {
  # Calculate indicator scores 3 ways
  # Return correlation matrix and classification agreement
  # Flag if weighting substantially changes results
}
```

---

### **Phase 3: Question Coverage & Gap Analysis**

**Objective**: Determine if you're asking the right questions (not too many, too few, gaps)

#### **3.1 Content Coverage Evaluation**
- [ ] **Map questions to indicators**
  - Questions per indicator (you have 38 questions, how many indicators?)
  - Recommended: 3-5 items per construct for reliability
  
- [ ] **Redundancy Analysis**
  - Inter-item correlations > 0.80 suggest redundancy
  - Use `psych::lowerCor()` to create correlation matrix
  - Flag highly correlated question pairs for potential removal

#### **3.2 Difficulty/Discrimination Analysis**
- [ ] **Item difficulty** (from your EDA work)
  - Questions with ceiling/floor effects (>80% same response)
  - You already have this data from your descriptive statistics

- [ ] **Item discrimination**
  - Compare responses in high vs. low scoring projects
  - Questions that don't differentiate may not be useful

#### **3.3 Coverage Gap Identification**
- [ ] **Compare factor structure to conceptual model**
  - Do EFA results align with expert-defined indicators?
  - Are there "orphan" questions that don't fit the structure?
  - Are there theoretical areas underrepresented?

---

### **Phase 4: Contextual Stability Analysis**

**Objective**: Assess whether question/indicator performance varies by context

#### **4.1 Measurement Invariance Testing**
- [ ] **Multi-group factor analysis**
  - Test if factor structure holds across:
    - Program types (Military vs. Civil Works)
    - Milestones (15%, 35%, 65%, 95%, 100%)
  - This is advanced; consider `lavaan` package if needed

#### **4.2 Differential Item Functioning (DIF)**
- [ ] **Check if questions "work differently" in different contexts**
  - Use `lordif` package or `mirt` package
  - Flag questions with substantial DIF as potentially biased

---

### **Phase 5: Synthesis & Recommendations**

#### **5.1 Create Decision Framework**
- [ ] **Question retention criteria**
  - Factor loading threshold
  - Item-total correlation threshold
  - Redundancy threshold
  - Theoretical importance

- [ ] **Weighting decision criteria**
  - If weighted vs. unweighted correlation < 0.90, consider weighting
  - If factor loadings vary substantially (range > 0.30), weighting may help

#### **5.2 Generate Recommendations Report**
- [ ] **Questions to retain** (with justification)
- [ ] **Questions to remove** (with justification)
- [ ] **Questions to revise** (with specific suggestions)
- [ ] **Coverage gaps to address**
- [ ] **Weighting scheme recommendation**

---

## **Additional Best Practice Considerations You May Be Missing**

### **1. Validity Evidence**
- **Convergent validity**: Do questions measuring the same indicator correlate?
- **Discriminant validity**: Do questions measuring different indicators show lower correlations?
- Consider multitrait-multimethod analysis (`psych::mtmm()`)

### **2. Response Bias Analysis**
- **Acquiescence bias**: Tendency to agree regardless of content
- **Social desirability**: Tendency to present in favorable light
- Check for systematic patterns in extreme responding

### **3. Temporal Stability** (if you get Year 2 data)
- Test-retest reliability for projects that complete multiple milestones
- Assess stability of factor structure across years

### **4. Criterion Validity** (future work)
- Do EQRI scores correlate with actual engineering quality outcomes?
- Consider linking to:
  - Post-construction performance
  - Cost overruns
  - Schedule delays
  - Safety incidents

### **5. Sample Size Considerations**
Your current sample sizes (visible in your reliability analysis):
- **Rule of thumb for factor analysis**: 
  - Minimum 5 observations per item (5 × 38 = 190)
  - Better: 10 observations per item (10 × 38 = 380)
  - You may need to aggregate across contexts for stable factor solutions

### **6. Missing Data Handling**
- Review your current approach to missing responses
- Consider multiple imputation if missing data is substantial
- `mice` package is well-documented: https://cran.r-project.org/web/packages/mice/

---

## **Suggested Implementation Timeline**

### **Sprint 1 (1-2 weeks): Factorability & Dimension**
- KMO, Bartlett's test
- Parallel analysis
- Initial EFA runs

### **Sprint 2 (1-2 weeks): Question Sensitivity**
- Factor loading analysis
- Weighting comparisons
- Visualization development

### **Sprint 3 (1 week): Coverage Analysis**
- Redundancy checks
- Gap identification
- Content mapping

### **Sprint 4 (1 week): Synthesis**
- Recommendations document
- Updated Quarto website sections

---

## **Integration with Your Existing Workflow**

Your project already has excellent infrastructure:

1. **Leverage your existing patterns**:
   - Follow the same function structure as `calculate_cronbach_alpha()` and `calculate_omega()`
   - Add functions like `run_factor_analysis()`, `assess_factorability()`, `compare_weightings()`
   - Create accompanying test files in `tests/testthat/`

2. **Extend your Quarto website**:
   - Add new pages: `03_factor_analysis.qmd`, `04_question_evaluation.qmd`
   - Use your existing plotting functions as templates

3. **Documentation discipline**:
   - Continue your AI session logging in `dev/sessions/`
   - Reference authoritative sources in function documentation
   - Add development notes as you've been doing

4. **Validation approach**:
   - Cross-check results against published examples in `psych` vignettes
   - Validate factor analysis assumptions explicitly

---

## **Key Decision Points to Discuss with Stakeholders**

1. **Should we use weighting?** → Determined by Phase 2 results
2. **Can we reduce question count?** → Determined by Phase 3 results
3. **Is the structure stable across contexts?** → Determined by Phase 4 results
4. **What changes for Year 2?** → Determined by Phase 5 synthesis

Would you like me to help you implement any specific component of this workplan, or would you like me to create initial function templates following your existing code patterns?

## @mpdougherty

I agree with the logic of this plan and like the chunking into sprints. Lets proceed with sprint 1.

## Copilot

# Sprint 1: Factorability & Dimension Analysis

Let me guide you through Sprint 1 by first surfacing the authoritative documentation, then proposing implementation aligned with your existing code patterns.

---

## **Essential Documentation to Review First**

### **Primary Resources for This Sprint**

1. **`psych` package Factor Analysis Documentation** (your project uses v2.4.6.26 based on typical installations)
   - [Official CRAN page](https://cran.r-project.org/web/packages/psych/index.html)
   - [Factor Analysis Functions Reference](https://cran.r-project.org/web/packages/psych/psych.pdf) - See pages on:
     - `KMO()` - Kaiser-Meyer-Olkin test (p. 139-140)
     - `cortest.bartlett()` - Bartlett's test (p. 57-58)
     - `fa.parallel()` - Parallel analysis (p. 92-95)
     - `fa()` - Factor analysis (p. 78-91)
   
2. **William Revelle's Factor Analysis Tutorial** (author of `psych`)
   - [An introduction to psychometric theory with applications in R](https://personality-project.org/r/book/)
   - [Factor analysis vignette](https://personality-project.org/r/psych/HowTo/factor.pdf)

3. **Key Methodological References**
   - **Parallel Analysis**: Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. *Psychometrika*, 30(2), 179-185.
   - **KMO**: Kaiser, H. F. (1974). An index of factorial simplicity. *Psychometrika*, 39(1), 31-36.
   - **General FA best practice**: Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. *Psychological Methods*, 4(3), 272-299.

---

## **Sprint 1 Implementation Plan**

### **Task 1.1: Assess Factorability**

**Functions to create:**
1. `assess_factorability()` - Main wrapper function
2. `plot_factorability_results()` - Visualization

Let me show you the implementation following your existing patterns:

````r
```r name=R/assess_factorability.R
#' @title Assess Data Factorability for Factor Analysis
#' @description Evaluate whether the correlation matrix is suitable for factor
#'   analysis using Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy and
#'   Bartlett's test of sphericity.
#' @param responses_df data.frame; Responses from get_responses_df()
#' @param program_name character; Program type ("Military" or "Civil Works")
#' @param milestone_name character; Project milestone
#' @returns list containing KMO and Bartlett test results with interpretation
#' @export
#' @references
#'   Kaiser, H. F. (1974). An index of factorial simplicity. 
#'   Psychometrika, 39(1), 31-36.
#'   
#'   Bartlett, M. S. (1950). Tests of significance in factor analysis. 
#'   British Journal of Psychology, 3(2), 77-85.
#'   
#'   Kaiser, H. F., & Rice, J. (1974). Little jiffy, mark IV. 
#'   Educational and Psychological Measurement, 34(1), 111-117.
#'   
#' @section Interpretation Guidelines:
#'   KMO overall values: < 0.50 unacceptable, 0.50-0.59 miserable, 
#'   0.60-0.69 mediocre, 0.70-0.79 middling, 0.80-0.89 meritorious, 
#'   >= 0.90 marvelous (Kaiser & Rice, 1974).
#'   
#'   Bartlett's test should be significant (p < .05) to reject the null
#'   hypothesis that the correlation matrix is an identity matrix.
#'   
#' @section Development Notes:
#'   This function was developed with AI assistance (GitHub Copilot, 2026-02-21).
#'   Human direction and oversight was provided at each implementation step. 
#'   See `dev/sessions/2026-02-21.md` for complete development context.
#' 
#' @importFrom psych KMO cortest.bartlett
#' @importFrom dplyr case_when
#' 
assess_factorability <- function(responses_df, 
                                program_name = NULL,
                                milestone_name = NULL) {
  
  # Validate inputs
  if (is.null(program_name)) {
    stop("program_name is required. Must be 'Military' or 'Civil Works'.")
  }
  if (is.null(milestone_name)) {
    stop("milestone_name is required (e.g., '95% (Final Design)').")
  }
  
  # Get wide format data using existing infrastructure
  wide_data <- get_wide_responses(
    responses_df,
    program_name = program_name,
    milestone_name = milestone_name
  )
  
  message(paste(
    "\nAssessing factorability for:", program_name, "×", milestone_name,
    "\nQuestions:", ncol(wide_data),
    "\nObservations:", nrow(wide_data)
  ))
  
  # Calculate correlation matrix (needed for both tests)
  cor_matrix <- cor(wide_data, use = "pairwise.complete.obs")
  
  # 1. Kaiser-Meyer-Olkin (KMO) Test
  # Per psych::KMO() documentation
  kmo_result <- tryCatch({
    psych::KMO(cor_matrix)
  }, error = function(e) {
    warning(paste("KMO calculation failed:", e$message))
    NULL
  })
  
  # 2. Bartlett's Test of Sphericity
  # Per psych::cortest.bartlett() documentation
  bartlett_result <- tryCatch({
    psych::cortest.bartlett(cor_matrix, n = nrow(wide_data))
  }, error = function(e) {
    warning(paste("Bartlett's test failed:", e$message))
    NULL
  })
  
  # Interpret KMO (Kaiser & Rice, 1974 criteria)
  kmo_interpretation <- if (!is.null(kmo_result)) {
    kmo_overall <- kmo_result$MSA
    
    case_when(
      kmo_overall >= 0.90 ~ "Marvelous",
      kmo_overall >= 0.80 ~ "Meritorious", 
      kmo_overall >= 0.70 ~ "Middling",
      kmo_overall >= 0.60 ~ "Mediocre",
      kmo_overall >= 0.50 ~ "Miserable",
      TRUE ~ "Unacceptable"
    )
  } else {
    "Unable to calculate"
  }
  
  # Interpret Bartlett (standard p < .05 criterion)
  bartlett_interpretation <- if (!is.null(bartlett_result)) {
    if (bartlett_result$p.value < 0.001) {
      "Highly significant (p < .001) - correlation matrix is factorable"
    } else if (bartlett_result$p.value < 0.05) {
      "Significant (p < .05) - correlation matrix is factorable"
    } else {
      "Not significant - correlation matrix may be an identity matrix (not factorable)"
    }
  } else {
    "Unable to calculate"
  }
  
  # Overall recommendation
  proceed_with_fa <- if (!is.null(kmo_result) && !is.null(bartlett_result)) {
    kmo_result$MSA >= 0.60 && bartlett_result$p.value < 0.05
  } else {
    FALSE
  }
  
  # Compile results
  results <- list(
    context = list(
      program = program_name,
      milestone = milestone_name
    ),
    sample = list(
      n_questions = ncol(wide_data),
      n_observations = nrow(wide_data)
    ),
    kmo = list(
      overall_msa = if (!is.null(kmo_result)) kmo_result$MSA else NA,
      item_msa = if (!is.null(kmo_result)) kmo_result$MSAi else NA,
      interpretation = kmo_interpretation,
      full_output = kmo_result
    ),
    bartlett = list(
      chi_square = if (!is.null(bartlett_result)) bartlett_result$chisq else NA,
      df = if (!is.null(bartlett_result)) bartlett_result$df else NA,
      p_value = if (!is.null(bartlett_result)) bartlett_result$p.value else NA,
      interpretation = bartlett_interpretation,
      full_output = bartlett_result
    ),
    recommendation = list(
      proceed_with_fa = proceed_with_fa,
      rationale = if (proceed_with_fa) {
        "Data meet minimum criteria for factor analysis (KMO >= 0.60 and significant Bartlett test)"
      } else if (!is.null(kmo_result) && kmo_result$MSA < 0.60) {
        paste0("KMO value (", round(kmo_result$MSA, 3), ") below recommended minimum of 0.60")
      } else if (!is.null(bartlett_result) && bartlett_result$p.value >= 0.05) {
        "Bartlett's test not significant - variables may be too uncorrelated"
      } else {
        "Unable to assess - calculation failed"
      }
    )
  )
  
  # Print summary
  message("\n=== Factorability Assessment Results ===")
  message(paste("KMO Overall MSA:", 
                round(results$kmo$overall_msa, 3), 
                "-", results$kmo$interpretation))
  message(paste("Bartlett's Test: χ²(", results$bartlett$df, ") =", 
                round(results$bartlett$chi_square, 2),
                ", p =", format.pval(results$bartlett$p_value)))
  message(paste("\nRecommendation:", results$recommendation$rationale))
  
  return(results)
}
```
````

### **Task 1.2: Determine Number of Factors (Parallel Analysis)**

````r
```r name=R/run_parallel_analysis.R
#' @title Determine Optimal Number of Factors via Parallel Analysis
#' @description Use Horn's parallel analysis to determine the number of factors
#'   to extract by comparing eigenvalues from actual data to those from random
#'   data with the same characteristics.
#' @param responses_df data.frame; Responses from get_responses_df()
#' @param program_name character; Program type ("Military" or "Civil Works")
#' @param milestone_name character; Project milestone
#' @param fa_method character; Factor extraction method. Options: "minres" 
#'   (minimum residual, default), "ml" (maximum likelihood), "pa" (principal axis)
#' @param n_iterations integer; Number of random datasets to generate for
#'   comparison. Default 20 (psych package default). More iterations = more stable.
#' @returns list containing parallel analysis results and recommendations
#' @export
#' @references
#'   Horn, J. L. (1965). A rationale and test for the number of factors in 
#'   factor analysis. Psychometrika, 30(2), 179-185.
#'   
#'   Hayton, J. C., Allen, D. G., & Scarpello, V. (2004). Factor retention 
#'   decisions in exploratory factor analysis: A tutorial on parallel analysis. 
#'   Organizational Research Methods, 7(2), 191-205.
#'   
#' @section How Parallel Analysis Works:
#'   1. Calculate eigenvalues from actual data correlation matrix
#'   2. Generate random data with same dimensions
#'   3. Calculate eigenvalues from random data
#'   4. Repeat step 2-3 many times
#'   5. Compare actual vs. mean random eigenvalues
#'   6. Retain factors where actual > random
#'   
#' @section Development Notes:
#'   This function was developed with AI assistance (GitHub Copilot, 2026-02-21).
#'   Human direction and oversight was provided at each implementation step. 
#'   See `dev/sessions/2026-02-21.md` for complete development context.
#' 
#' @importFrom psych fa.parallel
#' 
run_parallel_analysis <- function(responses_df,
                                 program_name = NULL,
                                 milestone_name = NULL,
                                 fa_method = "minres",
                                 n_iterations = 20) {
  
  # Validate inputs
  if (is.null(program_name)) {
    stop("program_name is required. Must be 'Military' or 'Civil Works'.")
  }
  if (is.null(milestone_name)) {
    stop("milestone_name is required (e.g., '95% (Final Design)').")
  }
  
  # Get wide format data
  wide_data <- get_wide_responses(
    responses_df,
    program_name = program_name,
    milestone_name = milestone_name
  )
  
  message(paste(
    "\nRunning parallel analysis for:", program_name, "×", milestone_name,
    "\nQuestions:", ncol(wide_data),
    "\nObservations:", nrow(wide_data),
    "\nMethod:", fa_method,
    "\nIterations:", n_iterations
  ))
  
  # Run parallel analysis
  # Per psych::fa.parallel() documentation
  parallel_result <- tryCatch({
    psych::fa.parallel(
      wide_data,
      fm = fa_method,
      fa = "fa",              # Factor analysis (not PCA)
      n.iter = n_iterations,
      main = paste("Parallel Analysis:", program_name, "×", milestone_name),
      quant = 0.95,          # 95th percentile of random eigenvalues
      sim = TRUE             # Show simulated eigenvalues
    )
  }, error = function(e) {
    warning(paste("Parallel analysis failed:", e$message))
    NULL
  })
  
  if (is.null(parallel_result)) {
    return(NULL)
  }
  
  # Extract key information
  # Per psych package documentation, fa.parallel returns:
  # - nfact: suggested number of factors
  # - ncomp: suggested number of components (PCA)
  # - fa.values: actual factor eigenvalues
  # - fa.sim: simulated factor eigenvalues (mean)
  
  n_factors_suggested <- parallel_result$nfact
  
  # Compile results
  results <- list(
    context = list(
      program = program_name,
      milestone = milestone_name
    ),
    sample = list(
      n_questions = ncol(wide_data),
      n_observations = nrow(wide_data)
    ),
    analysis_parameters = list(
      fa_method = fa_method,
      n_iterations = n_iterations
    ),
    eigenvalues = list(
      actual_fa = parallel_result$fa.values,
      simulated_fa = parallel_result$fa.sim,
      actual_pc = parallel_result$pc.values,  # For comparison
      simulated_pc = parallel_result$pc.sim   # For comparison
    ),
    recommendation = list(
      n_factors = n_factors_suggested,
      n_components = parallel_result$ncomp,  # PCA result for reference
      interpretation = paste0(
        "Parallel analysis suggests retaining ", n_factors_suggested,
        " factor(s). This is the number of factors with eigenvalues ",
        "exceeding those from random data."
      )
    ),
    full_output = parallel_result
  )
  
  # Print summary
  message("\n=== Parallel Analysis Results ===")
  message(paste("Suggested number of factors:", n_factors_suggested))
  message(paste("(Suggested number of components:", parallel_result$ncomp, ")"))
  message("\nFirst 5 actual vs. simulated eigenvalues:")
  
  comparison_df <- data.frame(
    Factor = 1:min(5, length(parallel_result$fa.values)),
    Actual = round(parallel_result$fa.values[1:min(5, length(parallel_result$fa.values))], 3),
    Simulated = round(parallel_result$fa.sim[1:min(5, length(parallel_result$fa.sim))], 3)
  )
  print(comparison_df)
  
  message("\nNote: The scree plot was displayed during analysis.")
  message("Factors to retain are those where Actual > Simulated (shown by plot).")
  
  return(results)
}
```
````

### **Task 1.3: Run Exploratory Factor Analysis**

````r
```r name=R/run_efa.R
#' @title Run Exploratory Factor Analysis
#' @description Perform exploratory factor analysis (EFA) to uncover the
#'   underlying factor structure of the questionnaire items.
#' @param responses_df data.frame; Responses from get_responses_df()
#' @param program_name character; Program type ("Military" or "Civil Works")
#' @param milestone_name character; Project milestone
#' @param nfactors integer; Number of factors to extract. If NULL, will use
#'   parallel analysis to determine automatically.
#' @param fm character; Factor extraction method. Options: "minres" (minimum
#'   residual, default), "ml" (maximum likelihood), "pa" (principal axis),
#'   "wls" (weighted least squares), "gls" (generalized least squares)
#' @param rotate character; Rotation method. Options: "oblimin" (oblique, default),
#'   "promax" (oblique), "varimax" (orthogonal), "none"
#' @returns list containing factor analysis results, loadings, and fit indices
#' @export
#' @references
#'   Revelle, W. (2024). psych: Procedures for Psychological, Psychometric, 
#'   and Personality Research. R package.
#'   
#'   Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). 
#'   Evaluating the use of exploratory factor analysis in psychological research. 
#'   Psychological Methods, 4(3), 272-299.
#'   
#'   Costello, A. B., & Osborne, J. (2005). Best practices in exploratory 
#'   factor analysis: Four recommendations for getting the most from your analysis. 
#'   Practical Assessment, Research, and Evaluation, 10(1), 7.
#'   
#' @section Rotation Choice:
#'   Oblique rotations (oblimin, promax) allow factors to correlate and are
#'   generally preferred for psychological/social science constructs which
#'   typically are related. Use orthogonal (varimax) only if you have strong
#'   theoretical reason to believe factors are uncorrelated.
#'   
#' @section Fit Indices Interpretation:
#'   - TLI (Tucker-Lewis Index): > 0.90 acceptable, > 0.95 good
#'   - RMSEA (Root Mean Square Error of Approximation): < 0.08 acceptable, < 0.05 good
#'   - RMSR (Root Mean Square Residual): < 0.05 good
#'   
#' @section Development Notes:
#'   This function was developed with AI assistance (GitHub Copilot, 2026-02-21).
#'   Human direction and oversight was provided at each implementation step. 
#'   See `dev/sessions/2026-02-21.md` for complete development context.
#' 
#' @importFrom psych fa fa.parallel
#' @importFrom dplyr case_when
#' 
run_efa <- function(responses_df,
                   program_name = NULL,
                   milestone_name = NULL,
                   nfactors = NULL,
                   fm = "minres",
                   rotate = "oblimin") {
  
  # Validate inputs
  if (is.null(program_name)) {
    stop("program_name is required. Must be 'Military' or 'Civil Works'.")
  }
  if (is.null(milestone_name)) {
    stop("milestone_name is required (e.g., '95% (Final Design)').")
  }
  
  # Get wide format data
  wide_data <- get_wide_responses(
    responses_df,
    program_name = program_name,
    milestone_name = milestone_name
  )
  
  # Determine number of factors if not specified
  if (is.null(nfactors)) {
    message("nfactors not specified. Running parallel analysis to determine...")
    parallel_result <- run_parallel_analysis(
      responses_df,
      program_name = program_name,
      milestone_name = milestone_name,
      fa_method = fm
    )
    
    if (!is.null(parallel_result)) {
      nfactors <- parallel_result$recommendation$n_factors
      message(paste("\nUsing", nfactors, "factors based on parallel analysis."))
    } else {
      stop("Parallel analysis failed and nfactors not specified. Please provide nfactors.")
    }
  }
  
  message(paste(
    "\nRunning EFA for:", program_name, "×", milestone_name,
    "\nQuestions:", ncol(wide_data),
    "\nObservations:", nrow(wide_data),
    "\nFactors:", nfactors,
    "\nMethod:", fm,
    "\nRotation:", rotate
  ))
  
  # Run factor analysis
  # Per psych::fa() documentation
  fa_result <- tryCatch({
    psych::fa(
      wide_data,
      nfactors = nfactors,
      fm = fm,
      rotate = rotate,
      scores = "regression",  # Factor scores via regression
      SMC = TRUE,            # Use squared multiple correlations as initial communality estimates
      warnings = TRUE,
      max.iter = 1000        # Maximum iterations for convergence
    )
  }, error = function(e) {
    warning(paste("Factor analysis failed:", e$message))
    NULL
  })
  
  if (is.null(fa_result)) {
    return(NULL)
  }
  
  # Extract and interpret fit indices
  # Per psych::fa() documentation, fit indices are in fa_result object
  fit_interpretation <- list(
    tli = case_when(
      is.na(fa_result$TLI) ~ "Not available",
      fa_result$TLI >= 0.95 ~ "Good (>= 0.95)",
      fa_result$TLI >= 0.90 ~ "Acceptable (>= 0.90)",
      TRUE ~ "Poor (< 0.90)"
    ),
    rmsea = case_when(
      is.na(fa_result$RMSEA[1]) ~ "Not available",
      fa_result$RMSEA[1] <= 0.05 ~ "Good (<= 0.05)",
      fa_result$RMSEA[1] <= 0.08 ~ "Acceptable (<= 0.08)",
      TRUE ~ "Poor (> 0.08)"
    ),
    rmsr = case_when(
      is.na(fa_result$rms) ~ "Not available",
      fa_result$rms <= 0.05 ~ "Good (<= 0.05)",
      TRUE ~ "Acceptable to Poor (> 0.05)"
    )
  )
  
  # Overall model fit assessment
  overall_fit <- if (!is.na(fa_result$TLI) && !is.na(fa_result$RMSEA[1])) {
    if (fa_result$TLI >= 0.90 && fa_result$RMSEA[1] <= 0.08) {
      "Acceptable to Good"
    } else {
      "Poor - consider different number of factors or method"
    }
  } else {
    "Unable to determine"
  }
  
  # Extract loadings and identify items by factor
  loadings_matrix <- fa_result$loadings
  class(loadings_matrix) <- "matrix"  # Convert from loadings class for easier manipulation
  
  # Identify primary factor for each item (highest loading)
  primary_factors <- apply(abs(loadings_matrix), 1, which.max)
  
  # Compile results
  results <- list(
    context = list(
      program = program_name,
      milestone = milestone_name
    ),
    sample = list(
      n_questions = ncol(wide_data),
      n_observations = nrow(wide_data)
    ),
    analysis_parameters = list(
      nfactors = nfactors,
      fm = fm,
      rotate = rotate
    ),
    fit_indices = list(
      tli = fa_result$TLI,
      tli_interpretation = fit_interpretation$tli,
      rmsea = fa_result$RMSEA[1],
      rmsea_ci_lower = fa_result$RMSEA[2],
      rmsea_ci_upper = fa_result$RMSEA[3],
      rmsea_interpretation = fit_interpretation$rmsea,
      rmsr = fa_result$rms,
      rmsr_interpretation = fit_interpretation$rmsr,
      overall_fit = overall_fit
    ),
    factor_loadings = list(
      loadings_matrix = loadings_matrix,
      primary_factors = primary_factors,
      communalities = fa_result$communality,
      uniquenesses = fa_result$uniquenesses
    ),
    factor_correlations = if (rotate %in% c("oblimin", "promax")) {
      fa_result$Phi  # Factor correlation matrix (only for oblique rotations)
    } else {
      NULL
    },
    variance_explained = list(
      ss_loadings = fa_result$Vaccounted["SS loadings", ],
      proportion_var = fa_result$Vaccounted["Proportion Var", ],
      cumulative_var = fa_result$Vaccounted["Cumulative Var", ],
      proportion_explained = fa_result$Vaccounted["Proportion Explained", ]
    ),
    full_output = fa_result
  )
  
  # Print summary
  message("\n=== EFA Results ===")
  message(paste("Number of factors:", nfactors))
  message(paste("Extraction method:", fm))
  message(paste("Rotation:", rotate))
  message("\nModel Fit:")
  message(paste("  TLI:", round(fa_result$TLI, 3), "-", fit_interpretation$tli))
  message(paste("  RMSEA:", round(fa_result$RMSEA[1], 3), 
                "[", round(fa_result$RMSEA[2], 3), ",", round(fa_result$RMSEA[3], 3), "]",
                "-", fit_interpretation$rmsea))
  message(paste("  RMSR:", round(fa_result$rms, 3), "-", fit_interpretation$rmsr))
  message(paste("\nOverall fit:", overall_fit))
  message("\nVariance Explained:")
  print(round(fa_result$Vaccounted, 3))
  
  if (!is.null(results$factor_correlations)) {
    message("\nFactor Correlations:")
    print(round(results$factor_correlations, 3))
  }
  
  return(results)
}
```
````

---

## **Testing Framework**

Following your existing pattern, create test files:

````r
```r name=tests/testthat/test-assess_factorability.R
testthat::test_that("assess_factorability requires program and milestone", {
  responses_df <- get_responses_df()
  
  testthat::expect_error(
    assess_factorability(responses_df),
    "program_name is required"
  )
  
  testthat::expect_error(
    assess_factorability(responses_df, "Military"),
    "milestone_name is required"
  )
})

testthat::test_that("assess_factorability returns valid structure", {
  responses_df <- get_responses_df()
  
  result <- assess_factorability(
    responses_df,
    program_name = "Military",
    milestone_name = "95% (Final Design)"
  )
  
  # Should return list
  testthat::expect_true(is.list(result))
  
  # Should have expected components
  expected_components <- c("context", "sample", "kmo", "bartlett", "recommendation")
  testthat::expect_true(all(expected_components %in% names(result)))
  
  # KMO should be between 0 and 1
  testthat::expect_true(result$kmo$overall_msa >= 0 && result$kmo$overall_msa <= 1)
  
  # Bartlett should have p-value
  testthat::expect_true(!is.na(result$bartlett$p_value))
})

testthat::test_that("run_parallel_analysis suggests reasonable number of factors", {
  responses_df <- get_responses_df()
  
  result <- run_parallel_analysis(
    responses_df,
    program_name = "Military",
    milestone_name = "95% (Final Design)"
  )
  
  # Should suggest at least 1 factor
  testthat::expect_true(result$recommendation$n_factors >= 1)
  
  # Should not suggest more factors than items
  testthat::expect_true(result$recommendation$n_factors <= result$sample$n_questions)
})

testthat::test_that("run_efa produces interpretable results", {
  responses_df <- get_responses_df()
  
  # Test with specified number of factors
  result <- run_efa(
    responses_df,
    program_name = "Military",
    milestone_name = "95% (Final Design)",
    nfactors = 3
  )
  
  # Should have loadings matrix
  testthat::expect_true(is.matrix(result$factor_loadings$loadings_matrix))
  
  # Number of factors should match request
  testthat::expect_equal(ncol(result$factor_loadings$loadings_matrix), 3)
  
  # Should have fit indices
  testthat::expect_true(!is.null(result$fit_indices$tli))
})
```
````

---

## **Visualization Functions**

````r
```r name=R/plot_parallel_analysis.R
#' @title Plot Parallel Analysis Results
#' @description Create a scree plot comparing actual vs. simulated eigenvalues
#'   from parallel analysis.
#' @param parallel_results list; Output from run_parallel_analysis()
#' @returns ggplot2 object
#' @export
#' @importFrom ggplot2 ggplot aes geom_line geom_point labs theme_minimal
#'   scale_color_manual geom_hline
#' @importFrom tidyr pivot_longer
#' 
plot_parallel_analysis <- function(parallel_results) {
  
  # Extract eigenvalues
  n_values <- length(parallel_results$eigenvalues$actual_fa)
  
  plot_data <- data.frame(
    Factor = 1:n_values,
    Actual = parallel_results$eigenvalues$actual_fa,
    Simulated = parallel_results$eigenvalues$simulated_fa[1:n_values]
  ) %>%
    pivot_longer(cols = c("Actual", "Simulated"), 
                names_to = "Source", 
                values_to = "Eigenvalue")
  
  # Create plot
  p <- ggplot(plot_data, aes(x = Factor, y = Eigenvalue, 
                             color = Source, linetype = Source)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    geom_hline(yintercept = 1, linetype = "dashed", color = "gray50") +
    scale_color_manual(values = c("Actual" = "#2E86AB", "Simulated" = "#A23B72")) +
    labs(
      title = paste("Parallel Analysis:", 
                   parallel_results$context$program, "×",
                   parallel_results$context$milestone),
      subtitle = paste("Suggested factors:", 
                      parallel_results$recommendation$n_factors),
      x = "Factor Number",
      y = "Eigenvalue",
      color = "Data Source",
      linetype = "Data Source"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      legend.position = "bottom"
    )
  
  return(p)
}
```
````

````r
```r name=R/plot_factor_loadings.R
#' @title Plot Factor Loadings Heatmap
#' @description Visualize factor loadings as a heatmap to identify which
#'   questions load strongly on which factors.
#' @param efa_results list; Output from run_efa()
#' @param loading_threshold numeric; Minimum absolute loading to display.
#'   Default 0.30. Loadings below this are grayed out.
#' @returns ggplot2 object
#' @export
#' @importFrom ggplot2 ggplot aes geom_tile geom_text labs theme_minimal
#'   scale_fill_gradient2 theme element_text
#' @importFrom tidyr pivot_longer
#' 
plot_factor_loadings <- function(efa_results, loading_threshold = 0.30) {
  
  # Extract loadings matrix
  loadings <- efa_results$factor_loadings$loadings_matrix
  
  # Convert to data frame for plotting
  loadings_df <- as.data.frame(loadings)
  loadings_df$Question <- rownames(loadings)
  
  # Pivot to long format
  loadings_long <- loadings_df %>%
    pivot_longer(
      cols = -Question,
      names_to = "Factor",
      values_to = "Loading"
    ) %>%
    mutate(
      Loading_Label = ifelse(abs(Loading) >= loading_threshold,
                            sprintf("%.2f", Loading),
                            ""),
      Loading_Strength = case_when(
        abs(Loading) >= 0.70 ~ "Strong",
        abs(Loading) >= 0.40 ~ "Moderate",
        abs(Loading) >= loading_threshold ~ "Weak",
        TRUE ~ "Negligible"
      )
    )
  
  # Create heatmap
  p <- ggplot(loadings_long, aes(x = Factor, y = Question, fill = Loading)) +
    geom_tile(color = "white", linewidth = 0.5) +
    geom_text(aes(label = Loading_Label), size = 3) +
    scale_fill_gradient2(
      low = "#A23B72",
      mid = "white",
      high = "#2E86AB",
      midpoint = 0,
      limits = c(-1, 1),
      name = "Loading"
    ) +
    labs(
      title = "Factor Loadings Heatmap",
      subtitle = paste(efa_results$context$program, "×",
                      efa_results$context$milestone),
      x = "Factor",
      y = "Question",
      caption = paste("Loadings below", loading_threshold, "not labeled")
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      axis.text.y = element_text(size = 8),
      panel.grid = element_blank()
    )
  
  return(p)
}
```
````

---

## **Wrapper Function for Complete Sprint 1 Analysis**

````r
```r name=R/run_complete_fa_assessment.R
#' @title Run Complete Factor Analysis Assessment
#' @description Execute all Sprint 1 tasks: factorability assessment,
#'   parallel analysis, and EFA with multiple methods for comparison.
#' @param responses_df data.frame; Responses from get_responses_df()
#' @param program_name character; Program type
#' @param milestone_name character; Project milestone
#' @param methods character vector; Factor extraction methods to compare.
#'   Default c("minres", "ml", "pa")
#' @param rotations character vector; Rotation methods to compare.
#'   Default c("oblimin", "varimax")
#' @returns list containing all assessment results
#' @export
#' 
run_complete_fa_assessment <- function(responses_df,
                                      program_name,
                                      milestone_name,
                                      methods = c("minres", "ml", "pa"),
                                      rotations = c("oblimin", "varimax")) {
  
  message("====================================")
  message("SPRINT 1: FACTOR ANALYSIS ASSESSMENT")
  message("====================================\n")
  
  # Task 1.1: Factorability
  message("\n--- Task 1.1: Assessing Factorability ---")
  factorability <- assess_factorability(responses_df, program_name, milestone_name)
  
  if (!factorability$recommendation$proceed_with_fa) {
    warning("Data may not be suitable for factor analysis. Proceeding with caution.")
  }
  
  # Task 1.2: Parallel Analysis
  message("\n--- Task 1.2: Determining Number of Factors ---")
  parallel <- run_parallel_analysis(responses_df, program_name, milestone_name)
  
  suggested_nfactors <- parallel$recommendation$n_factors
  
  # Task 1.3: EFA with multiple methods
  message("\n--- Task 1.3: Running EFA with Multiple Methods ---")
  efa_results <- list()
  
  for (method in methods) {
    for (rotation in rotations) {
      key <- paste(method, rotation, sep = "_")
      message(paste("\nTrying:", method, "with", rotation, "rotation"))
      
      efa_results[[key]] <- run_efa(
        responses_df,
        program_name,
        milestone_name,
        nfactors = suggested_nfactors,
        fm = method,
        rotate = rotation
      )
    }
  }
  
  # Compile all results
  results <- list(
    factorability = factorability,
    parallel_analysis = parallel,
    efa_results = efa_results,
    recommendations = list(
      proceed_with_fa = factorability$recommendation$proceed_with_fa,
      suggested_nfactors = suggested_nfactors,
      best_method = "Compare fit indices across methods to determine best solution"
    )
  )
  
  message("\n====================================")
  message("SPRINT 1 COMPLETE")
  message("====================================")
  message("\nNext steps:")
  message("1. Review fit indices to select best EFA solution")
  message("2. Examine factor loadings to interpret factors")
  message("3. Compare to theoretical indicator structure")
  
  return(results)
}
```
````

---

## **Implementation Checklist for Sprint 1**

### **Week 1: Core Functions**
- [ ] Create `R/assess_factorability.R`
- [ ] Create `R/run_parallel_analysis.R`
- [ ] Create `R/run_efa.R`
- [ ] Create `R/run_complete_fa_assessment.R`
- [ ] Create corresponding test files in `tests/testthat/`
- [ ] Run `devtools::document()` to update NAMESPACE
- [ ] Run `devtools::test()` to verify tests pass

### **Week 2: Visualization & Documentation**
- [ ] Create `R/plot_parallel_analysis.R`
- [ ] Create `R/plot_factor_loadings.R`
- [ ] Create `03_factor_analysis.qmd` for Quarto website
- [ ] Document session in `dev/sessions/2026-02-21.md`
- [ ] Run analysis on all contexts (Military/Civil Works × all milestones)
- [ ] Generate visualizations and add to Quarto site
- [ ] Update `_quarto.yml` to include new page

---

## **Sample Quarto Page Structure**

````markdown
```qmd name=03_factor_analysis.qmd
---
title: "Factor Analysis"
---

```{r setup}
#| echo: false
#| message: false
#| warning: false

library(EQRIanalysis)
library(dplyr)
library(ggplot2)

responses_df <- get_responses_df()
```

## Overview

This section presents exploratory factor analysis (EFA) results to evaluate the underlying dimensional structure of the EQRI questionnaire.

**Key Questions:**
- Is the data suitable for factor analysis?
- How many underlying factors (dimensions) best explain the questionnaire responses?
- Which questions load on which factors?
- How does the factor structure align with the theoretical indicators?

## Methodology

Following best practice recommendations from Fabrigar et al. (1999) and Costello & Osborne (2005), we:

1. **Assessed factorability** using Kaiser-Meyer-Olkin (KMO) and Bartlett's test
2. **Determined optimal number of factors** using parallel analysis (Horn, 1965)
3. **Conducted EFA** with multiple extraction methods and rotations
4. **Evaluated model fit** using TLI, RMSEA, and RMSR indices

---

## Results by Context

```{r run-analysis}
#| echo: false
#| message: false
#| warning: false
#| cache: true

# Run for Military × 95%
military_95_results <- run_complete_fa_assessment(
  responses_df,
  program_name = "Military",
  milestone_name = "95% (Final Design)"
)

# Run for Civil Works × 95%
cw_95_results <- run_complete_fa_assessment(
  responses_df,
  program_name = "Civil Works",
  milestone_name = "95% (Final Design)"
)
```

### Military Programs - 95% Design

#### Factorability Assessment

```{r military-factorability}
#| echo: false

kmo_value <- military_95_results$factorability$kmo$overall_msa
kmo_interp <- military_95_results$factorability$kmo$interpretation
```

**KMO Measure:** `r round(kmo_value, 3)` (`r kmo_interp`)

**Bartlett's Test:** χ��(`r military_95_results$factorability$bartlett$df`) = `r round(military_95_results$factorability$bartlett$chi_square, 2)`, p < .001

**Conclusion:** `r military_95_results$factorability$recommendation$rationale`

#### Parallel Analysis

```{r military-parallel-plot}
#| echo: false
#| fig-width: 8
#| fig-height: 5

plot_parallel_analysis(military_95_results$parallel_analysis)
```

**Suggested Factors:** `r military_95_results$parallel_analysis$recommendation$n_factors`

#### Factor Loadings

```{r military-loadings}
#| echo: false
#| fig-width: 10
#| fig-height: 12

# Use the minres oblimin solution as primary
plot_factor_loadings(military_95_results$efa_results$minres_oblimin)
```

#### Model Fit Comparison

```{r military-fit-table}
#| echo: false

# Extract fit indices from all EFA runs
fit_comparison <- data.frame(
  Method = names(military_95_results$efa_results),
  TLI = sapply(military_95_results$efa_results, function(x) round(x$fit_indices$tli, 3)),
  RMSEA = sapply(military_95_results$efa_results, function(x) round(x$fit_indices$rmsea, 3)),
  RMSR = sapply(military_95_results$efa_results, function(x) round(x$fit_indices$rmsr, 3))
)

knitr::kable(fit_comparison, caption = "Model Fit Comparison")
```

---

### Civil Works Programs - 95% Design

[Repeat same structure as Military]

---

## Interpretation & Recommendations

[Add your interpretation after reviewing results]

---

## References

Costello, A. B., & Osborne, J. (2005). Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis. *Practical Assessment, Research, and Evaluation*, 10(1), 7.

Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. *Psychological Methods*, 4(3), 272-299.

Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. *Psychometrika*, 30(2), 179-185.
```
````

---

## **Next Steps**

1. **Implement the core functions** following the templates above
2. **Review the `psych` package documentation** for any questions about function parameters
3. **Run initial tests** on one context (e.g., Military × 95% Design) to validate
4. **Document your session** in `dev/sessions/2026-02-21.md`
5. **Report back** with results or questions

Would you like me to help you with any specific implementation step, or do you have questions about the `psych` package documentation before proceeding?
